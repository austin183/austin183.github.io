<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BlogPostStyling with LLMs</title>
    <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.3/build/pure-min.css" />
    <link rel="stylesheet" href="../src/css/Style.css">
</head>

<body>
    <button class="theme-toggle" onclick="toggleTheme()">Toggle Theme</button>
    <script src="../src/js/themeSwitcher.js"></script>

    <h1>BlogPostStyling with LLMs</h1>

    <p>This project was to introduce more style and presentation with the Blog Posts on this website, testing how
        different LLMs would tackle the problem.</p>

    <h1>Results</h1>
    <p>The winner was DeepCoder, but it had some user experience issues that I plugged with the implementation provided
        later by Tulu3:70b. The conversations leading to the final mergeable code can be found in the folder with this
        ReadMe.</p>

    <h1>Rankings</h1>
    <p>More information about how the judging went can be found in <a
            href="https://github.com/austin183/austin183.github.io/blob/BlogPostStyle/Letting_llms_rank_results/Rankings/RankingSolutionsByPatch.md">This Github branch for the Rankings and
            related LLM Reasons for Rank</a>.</p>

    <h1>LLM Working Sessions</h1>

    <p>There is a video series of working with the LLMs at <a
            href="https://youtube.com/playlist?list=PLeowZcBbLxmIrGq7r0j4TQe5DZUSErlwA&si=Gl_nVDUL1eJ0_6cw">LLM Sessions
            by Mind of a Fighting Lion Enthusiast</a>.</p>

    <p>There is also collection of Pull Requests that reflect the changes offered by each LLM given the timebox and context I
        could provide.</p>

    <h2>Phi4</h2>
    <ul>
        <li><a href="https://ollama.com/library/phi4">Phi4 on Ollama</a></li>
        <li><a href="https://github.com/austin183/austin183.github.io/pull/1">Pull Request #1</a></li>
        <li><a href="https://youtu.be/PwHpRNKE_CI?si=3pYT1E26lFmUPk5m">Session 1 on Youtube - 45 mins</a></li>
        <li><a href="https://youtu.be/PwHpRNKE_CI?si=rqwHnSanI-JoX_5f">Session 2 on Youtube - 30 mins</a></li>
    </ul>

    <p>It did not go very well, but part of that could have been me ramping up with what I wanted from the experiments
    </p>

    <h2>DeepCoder</h2>
    <ul>
        <li><a href="https://ollama.com/library/deepcoder">DeepCoder on Ollama</a></li>
        <li><a href="https://github.com/austin183/austin183.github.io/pull/2">Pull Request #2</a></li>
        <li><a href="https://youtu.be/e8g7e-XE8Eo?si=sMEYuGJXU60eP_4H">Session 1 on Youtube - 48 mins</a></li>
        <li><a href="https://youtu.be/7vBNuTRz-tQ?si=1oIzsKQ72iFyoE-C">Session 2 on Youtube - 27 mins</a></li>
    </ul>

    <p>This one was the winner, so it went pretty well.</p>

    <h2>Gemma3</h2>
    <ul>
        <li><a href="https://ollama.com/library/gemma3">Gemma3 on Ollama</a></li>
        <li><a href="https://github.com/austin183/austin183.github.io/pull/3">Pull Request #3</a></li>
        <li><a href="https://youtu.be/rIX8PLN32a0?si=dG0VT5adPQhsrJvO">Session 1 on Youtube - 19 mins</a></li>
        <li><a href="https://youtu.be/-MvyL5S5XsI?si=bnc0zNshF8MVMoRG">Session 2 on Youtube - 14 mins</a></li>
    </ul>

    <p>It was fast to respond, but it often had the wrong answer the first time around. The code did not fare well in
        the Rankings either.</p>

    <h2>Nemotron</h2>
    <ul>
        <li><a href="https://ollama.com/library/nemotron">Nemotron on Ollama</a></li>
        <li><a href="https://github.com/austin183/austin183.github.io/pull/4">Pull Request #4</a></li>
        <li><a href="https://youtu.be/prWYFBV0SVE?si=-uhjsqO6_EO-xvdX">Session 1 on Youtube - 36 mins</a></li>
        <li><a href="https://youtu.be/YtbjNq5KiGQ?si=yt7e4x_vBaZXVJAG">Session 2 on Youtube - 35 mins</a></li>
    </ul>

    <p>It did pretty well, but it could not figure out how to fix the Text Flow issue at the end, and I borrowed the
        answer from another LLM.</p>

    <h2>Qwen3</h2>
    <ul>
        <li><a href="https://ollama.com/library/qwen3:32b">Qwen3 on Ollama</a></li>
        <li><a href="https://github.com/austin183/austin183.github.io/pull/5">Pull Request #5</a></li>
        <li><a href="https://youtu.be/O9TUZol3Jmc?si=_85HlP__cyLTm8-7">Session 1 on Youtube - 54 mins</a></li>
        <li><a href="https://youtu.be/T9EwLRMFpvs?si=Di-0u199bwwU_sLx">Session 2 on Youtube - 23 mins</a></li>
    </ul>

    <p>It got things wrong a whole lot, and it did not do well in the rankings.</p>

    <h2>Tulu3</h2>
    <ul>
        <li><a href="https://ollama.com/library/tulu3:70b">Tulu3 on Ollama</a></li>
        <li><a href="https://github.com/austin183/austin183.github.io/pull/6">Pull Request #6</a></li>
        <li><a href="https://youtu.be/ghcmkFK30Es">Session 1 on Youtube - 30 mins</a></li>
    </ul>

    <p>Got everything done in first go without many mistakes. Felt good and productive.</p>

    <h2>Llama 3.3</h2>
    <ul>
        <li><a href="https://ollama.com/library/llama3.3:70b">Llama 3.3 on Ollama</a></li>
        <li><a href="https://github.com/austin183/austin183.github.io/pull/7">Pull Request #7</a></li>
        <li><a href="https://youtu.be/J44kalF2NH0">Session 1 on Youtube - 46 mins</a></li>
    </ul>

    <p>Relatively quick, but I ran into several issues that needed to be fixed before getting to the right output.</p>

    <h2>DeepSeek</h2>
    <ul>
        <li><a href="https://ollama.com/library/deepseek-r1:70b">DeepSeek on Ollama</a></li>
        <li><a href="https://github.com/austin183/austin183.github.io/pull/8">Pull Request #8</a></li>
        <li><a href="https://youtu.be/lg4FsLYfd3g">Session 1 on Youtube - 51 mins</a></li>
    </ul>

    <p>This one did work with Roo, although it encountered bugs which forced me to switch over to Continue. Got through
        all the experiment stages in one session, although it did get stuck on the Back button persistence issue for a
        long time.</p>


    <h1>Final Thoughts</h1>

    <h2>The LLMs</h2>
    <p>It is good to have a mix of LLMs to query. I liked working with Tulu3 the most, but I think several LLMs have
        merit. In future experiments, I will probably drop phi4, qwen3, nemotron, and gemma3 because they did not feel
        good to work with.</p>

    <h2>My Scientific Process</h2>
    <p>My scientific process is not very consistent for each iteration. In future experiments, I could spend more time
        in the Preparation stage to try to get a more consistent test, where I give less input in each loop.</p>

    <ul>
        <li><strong>Customize Roo's Instructions:</strong> Work better with local open LLMs</li>
        <li><strong>Write an end-to-end test:</strong> Around the final product I want to see so the LLM can work
            towards that goal without me introducing variations in the experiment.</li>
    </ul>

</body>

</html>