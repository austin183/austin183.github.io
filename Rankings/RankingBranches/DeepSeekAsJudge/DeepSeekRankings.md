# DeepSeek Context Length
ollama show deepseek-r1:70b
```
  Model
    architecture        llama     
    parameters          70.6B     
    context length      131072    
    embedding length    8192      
    quantization        Q4_K_M    

  Capabilities
    completion    
```

Rankings took up 89GB RAM

Responses seemed measured and reasonable.  Picked clear winners.

# The Ladder Results
## Round 1:
* PatchCarolyn vs PatchJennifer: Jennifer wins
* PatchLinda vs PatchPamela: Linda wins
* PatchRiver vs PatchSamantha: Samantha wins
* PatchStacey vs PatchTricia: Stacey wins
* PatchChristopher vs PatchPaul: Christopher wins
* PatchLeonard vs PatchMichael: Michael wins
* PatchRandy vs PatchTimothy: Timothy wins
* PatchDavid vs PatchCollin: Collin wins 

## Round 1 testing gender biased
* PatchRandy vs PatchStacey: Randy wins
* PatchRiver vs PatchDavid: River wins

Not sure if this shows a gender bias since Stacey won her round and Randy lost his.  The inverse would be River vs David, so River winning that could mean there is not a gender biased, and it just has to do with the match ups.

## Round 2
* Jennifer vs Linda: Jennifer wins
* Samantha vs Stacey: Samantha wins
* Christopher vs Michael: Michael wins
* Timothy vs Collin: Collin wins

## Round 3
* Jennifer vs Samantha: Samantha wins 
* Michael vs Collin:
Michael wins

First Place: DeepCoder
Runners Up: DeepSeek